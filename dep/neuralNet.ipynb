{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains just a neural net that I use to test the driver installations and such\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_pytorch = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader_pytorch = torch.utils.data.DataLoader(mnist_pytorch, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the transforms and load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BasicCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 0.608\n",
      "[Epoch 1, Batch 200] loss: 0.148\n",
      "[Epoch 1, Batch 300] loss: 0.094\n",
      "[Epoch 1, Batch 400] loss: 0.083\n",
      "[Epoch 2, Batch 100] loss: 0.062\n",
      "[Epoch 2, Batch 200] loss: 0.052\n",
      "[Epoch 2, Batch 300] loss: 0.057\n",
      "[Epoch 2, Batch 400] loss: 0.049\n",
      "[Epoch 3, Batch 100] loss: 0.037\n",
      "[Epoch 3, Batch 200] loss: 0.038\n",
      "[Epoch 3, Batch 300] loss: 0.034\n",
      "[Epoch 3, Batch 400] loss: 0.041\n",
      "[Epoch 4, Batch 100] loss: 0.026\n",
      "[Epoch 4, Batch 200] loss: 0.027\n",
      "[Epoch 4, Batch 300] loss: 0.026\n",
      "[Epoch 4, Batch 400] loss: 0.032\n",
      "[Epoch 5, Batch 100] loss: 0.020\n",
      "[Epoch 5, Batch 200] loss: 0.022\n",
      "[Epoch 5, Batch 300] loss: 0.018\n",
      "[Epoch 5, Batch 400] loss: 0.022\n",
      "[Epoch 6, Batch 100] loss: 0.017\n",
      "[Epoch 6, Batch 200] loss: 0.015\n",
      "[Epoch 6, Batch 300] loss: 0.020\n",
      "[Epoch 6, Batch 400] loss: 0.017\n",
      "[Epoch 7, Batch 100] loss: 0.013\n",
      "[Epoch 7, Batch 200] loss: 0.014\n",
      "[Epoch 7, Batch 300] loss: 0.014\n",
      "[Epoch 7, Batch 400] loss: 0.014\n",
      "[Epoch 8, Batch 100] loss: 0.008\n",
      "[Epoch 8, Batch 200] loss: 0.011\n",
      "[Epoch 8, Batch 300] loss: 0.011\n",
      "[Epoch 8, Batch 400] loss: 0.009\n",
      "[Epoch 9, Batch 100] loss: 0.009\n",
      "[Epoch 9, Batch 200] loss: 0.006\n",
      "[Epoch 9, Batch 300] loss: 0.007\n",
      "[Epoch 9, Batch 400] loss: 0.008\n",
      "[Epoch 10, Batch 100] loss: 0.007\n",
      "[Epoch 10, Batch 200] loss: 0.010\n",
      "[Epoch 10, Batch 300] loss: 0.012\n",
      "[Epoch 10, Batch 400] loss: 0.010\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(10):  # number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.22%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demnistify-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
